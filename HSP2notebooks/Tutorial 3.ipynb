{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/HSP2.png\" />\n",
    "This Jupyter Notebook Copyright 2017 by RESPEC, INC.  All rights reserved.\n",
    "\n",
    "$\\textbf{HSP}^{\\textbf{2}}\\ \\text{and}\\ \\textbf{HSP2}\\ $ Copyright 2017 by RESPEC INC. and released under this [License](LegalInformation/License.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUTORIAL 3: Running HSP$^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to run $\\textbf{HSP}^\\textbf{2}$ . It also provides an example workflow to demonstrate additional capabilities of $\\textbf{HSP}^\\textbf{2}$ .\n",
    "\n",
    "**Tutorial Contents**\n",
    "\n",
    " + Section 1: [Running HSP$^2$](#section1)\n",
    " + Section 2: [Tools for Running HSP$^2$](#section2)\n",
    "    + [SAVE tables to select time series to save](#usersave)\n",
    "    + [Save All Calculations](#saveall)\n",
    "    + [Check the HDF File  ](#checkstuff)\n",
    "    + [Make the HDF5 file Smaller](#makesmall)\n",
    " + Section 3: [Techniques for Efficient Simulation](#section3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Python imports  and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import site\n",
    "site.addsitedir(os.getcwd().rsplit('\\\\',1)[0] + '\\\\')  # adds your path to the HSP2 software.\n",
    "\n",
    "hdfname = os.path.join('TutorialData', 'Tutorial.h5')\n",
    "\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows    = 18\n",
    "pd.options.display.max_columns = 10\n",
    "pd.options.display.float_format = '{:.2f}'.format  # display 2 digits after the decimal point\n",
    "\n",
    "import HSP2\n",
    "import HSP2tools\n",
    "\n",
    "HSP2tools.reset_tutorial()    # make a new copy of the tutorial's data\n",
    "HSP2tools.versions()          # display version information below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Run $\\textbf{HSP}^\\textbf{2}$ <a id='section1'></a>\n",
    "\n",
    "This tutorial assumes that the HDF5 file representing the watershed exists. Tutorial 4 discusses how to create the watershed HDF5 file from the legacy UCI and WDM files.  Otherwise, a future $\\textbf{HSP}^\\textbf{2}$ GUI (graphics user interface) tool can be used to directly create an HDF5 file for a new watershed.\n",
    "\n",
    "\n",
    "Here is how to run $\\textbf{HSP}^\\textbf{2}$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HSP2.run(hdfname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all there is!\n",
    "\n",
    "The first time HSP2 is run in a Python session, Numba will perform a just in time (JIT) compilation. This takes about the same time regardless if the run is for a simple or complex watershed. Afterward, subsequent runs are much faster. \n",
    "\n",
    "Go back and rerun the cell above to see the difference.\n",
    "\n",
    "Many factors such as background system tasks will alter the run time. Saving more calculated resuts than the default will also increase the run time.\n",
    "\n",
    "**NOTE** The pink warning messages above can be ignored. The bug causing this in the h5py library has been fixed, but not yet released."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Examine the HDF5 file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use HDFView or HDFCompass to examine the **RUN_INFO** HDF5 directory. It contains the run log (as printed above during the run). It also contains the table **SOFTWARE_VERSION_TABLE** which shows the same table shown in the cell *Required Python imports and settings* above.\n",
    "\n",
    "The time series calculated during the run are saved in the **RESULTS** group.\n",
    "\n",
    "Only the time series marked to be saved in the associated **SAVE** tables are actually stored.  By default time series which can be trivially computed are not saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section Summary**\n",
    "\n",
    " + Demonstrated how to run HSP2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Tools for Running HSP$^2$ <a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE tables are used to select time series to save to the HDF5 file<a id='saveall'></a>\n",
    "By default, a minimum number of timeseries necessary to run the watershed are automatically saved. The user can modify the entries in the SAVE tables for their needs.\n",
    "\n",
    "This cell will show the names of the calculated time series save by default for PERLND SNOW for segment P001 from the run above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(hdfname, '/RESULTS/PERLND_P001/SNOW')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the user wants to also save the ALBEDO timeseries.\n",
    "\n",
    "Read the associated SAVE table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsave= pd.read_hdf(hdfname, '/PERLND/SNOW/SAVE')\n",
    "dfsave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This changes a single value for a specifc segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsave.loc['P001', 'ALBEDO'] = True\n",
    "dfsave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if you wanted to ALBEDO for all segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsave.ALBEDO = True\n",
    "dfsave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, in the case of test10, there is only one PERLND segment. But this shows the technique.\n",
    "\n",
    "Now put the SAVE file back into the HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsave.to_hdf(hdfname, '/PERLND/SNOW/SAVE', data_columns=True, format='t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now rerun the simulation and check which time series were saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSP2.run(hdfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(hdfname, '/RESULTS/PERLND_P001/SNOW')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save All Calculations <a id='saveall'></a>\n",
    "\n",
    "A short cut is provided to save all calculations. It does NOT modify the SAVE tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSP2.run(hdfname, saveall=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that this increases the run time and makes the HDF5 file larger.\n",
    "\n",
    "Now look at the columns saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(hdfname, 'RESULTS/PERLND_P001/SNOW')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the HDF File  <a id='checkstuff'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\textbf{HSP}^\\textbf{2}$  maintains all the HSPF run-time warning and error messages. However, it does not perform all the HSPF checks for the consistency of flags and data (such as the rules on FTables.)\n",
    "\n",
    "An HDF5 file automatically created from (working) legacy UCI and WDM files, should be correct. But if the user creates the HDF5 file directly or modifications to an existing HDF5 file, errors might be introduced.\n",
    "\n",
    "THis tool is provided to perform these checks before the simulation is run whenever the user wants to confirm the the integrity of the HSF5 flags and data.\n",
    "\n",
    "    checkHDF(hdfname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSP2tools.checkHDF(hdfname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no errors are printed, everything passes.\n",
    "\n",
    "This tool is still in development to add additional checking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the HDF5 file Smaller <a id='makesmall'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** HDF5 version 1.10.0 is scheduled for release in the spring of 2016. It will have the capability to reclaim space dynamically and you might not need the following process. \n",
    "\n",
    "When running calculations over time, the HDF5 files will grow in size due uncoverd space or over estimated space needs within the HDF5 file. It is a good idea to occasionally repack the HDF5 file to make it smaller.\n",
    "\n",
    "The HDF Group provides a utility to repack HDF5 files. PyTables (used internally by Pandas) includes another utility to repack HDF5 files, ptrepack.\n",
    "\n",
    "The function is called as \n",
    "\n",
    "     ptrpack inputfile outputfile\n",
    "     \n",
    "It is an executable code (.exe) rather than a Python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpackedhdfname = os.path.join('TutorialData', 'tutorial.h5')\n",
    "repackedhdfname = os.path.join('TutorialData', 'tutorial_repacked.h5')\n",
    "!ptrepack {unpackedhdfname}  {repackedhdfname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at the sizes before and after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls TutorialData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The repacked HDF5 file still runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSP2.run(repackedhdfname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This run is usually a bit faster since the initial setup time is shorter for the tighter **/CONTROL** tables.  The setup time is not a large percentage of the run time for larger watersheds - so this isn't too significant.\n",
    "\n",
    "$\\textbf{HSP}^\\textbf{2}$ does spend a significant percentage of its time in writing the complete computed time series rather than the typical HSPF HBN files writing daily or monthly timeseries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Techniques for Efficient Simulation<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, don't duplicate the timeseries data\n",
    "Frequently, you will create multiple Notebooks for a single watershed for initial data processing tasks \n",
    "and to try different exploratory analysis (such as determing the impact of changing parameter values). \n",
    "\n",
    "#### It is not necessary to duplicate the time series data.\n",
    "\n",
    "$\\textbf{HSP}^\\textbf{2}$ makes it easy to have all the watershed's timeseries contained in just one master HDF5 file. Other simulations of this watershed\n",
    "can just access the time series from that master file. This can save significant storage and insures (from the **QA/QC** perspective) that all simulations are using the same data.  \n",
    "\n",
    "It is also possible to store timeseries data  in a intranet accessible repository of one or more HDF5 files. This data may include data from many projects\n",
    "and spanning longer time intervals than used for a specific project. Then all simulation HDF5 files use the same repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assume the *MASTER * watershed model HDF5 has been created.\n",
    "\n",
    "For this example, we will assume the tutorial.h5 is the \"master\" HDF5 file and contains the time series for this watershed.\n",
    "\n",
    "Now create a Notebook for a new simulation study.  (Actually, for this tutorial, just make a copy the usual tutorial Notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNotebook = os.path.join('TutorialData', 'myNotebook.h5')\n",
    "shutil.copyfile(hdfname, myNotebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, the **/RESULTS** directory is also removed to make the HDF5 file smaller. This is a common practice in large scale calibration simulations when many (perhaps thousands) of individual simulation files are required.\n",
    "\n",
    "This is the first example of deleting data from an HDF5 file in these tutorials. **Note**, there is **no**  warning (like a \"are you really sure you want to delete this?\") Deleting a group (directory) in an HDF5 file will delete **ALL** data and groups below it.\n",
    "\n",
    "The **del** command is the standard Python method to destroy the following object and works with HDF5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.get_store(myNotebook) as store:\n",
    "    del store['/TIMESERIES']\n",
    "    del store['/RESULTS']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to point the simulation HDF5 file, mystudy.h5, to the timeseries data in the master HDF5\n",
    "file.\n",
    "\n",
    "This is easily done by reading the EXT_SOURCES table and putting the name of the master file (full path if not located in the same directory)\n",
    "into the **HDF_Name** column. The original **\\*** in that column indicates the data is found in the same HDF5 file as the EXT_SOURCES table.\n",
    "\n",
    "(Of course, the time series may be distributed accross any number of HDF5 files. Just put the appropiated HDF5 names in each row of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(myNotebook, '/CONTROL/EXT_SOURCES')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now change the source of the data from the current HDF5 file (designated by an asterisk in the SVOL column) to the other HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SVOL'] = hdfname\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save it back to the HDF5 file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_hdf(myNotebook, '/CONTROL/EXT_SOURCES', data_columns=True, format='table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use HDFView or HDFCompass to view the mystudy.h5 file. Note that the /Timeseries directory has been deleted. The /RESULTS directory has been deleted to prevent the user from accidently thinking they were computed by the data in the mystudy.h5.  Good **QA/QC** practice would be to copy from the master and delete both directorys in one script to insure they stay in synch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the mystudy.h5 simulation to show that it works pointing to timeseries data in the master, tutorial.h5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSP2.run(myNotebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the total run time is essentially identical for runs with local data or for runs fetching their data from a different HDF5 file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, repack the file periodically to eliminate wasted space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HDF5 utility currently does not compact an HDF5 file after the adding new tables, deleting tables, or appending to existing tables. After a series of such data operations, the HDF5 file will grow to be large.\n",
    "\n",
    "Section 2 discussed how to repack files. Remember to do this periodically for all HDF5 files.\n",
    "\n",
    "**NOTE** HDF5 version 1.10.0 was released in the spring of 2016. It has the capability to reclaim space dynamically and you might not need to do this when this is available in the Python libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third, use lower precision for storage and don't duplicate timebase information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't use too much precision if you want to save storage. All $\\textbf{HSP}^\\textbf{2}$  calculations are performed\n",
    "in double precision. Currently, computed time series are stored in single precision.\n",
    "\n",
    "$\\textbf{HSP}^\\textbf{2}$ puts all the computed time series for a single activity (like IWATER) into one table since they share a common timebase (index). If you create your own modules for $\\textbf{HSP}^\\textbf{2}$, avoid storing each result as a Pandas Series with its own timebase.\n",
    "\n",
    "$\\textbf{HSP}^\\textbf{2}$  stores FLAG data as 64 bit integers and stores INITIALIZATIONS, PARAMETERS, MONTHLY, and FTABLE tables in double precision.\n",
    "\n",
    "Perhaps, the precison of  $\\textbf{HSP}^\\textbf{2}$ items can become a user configuration - we are looking for user feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forth, use data compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 has compression options to save storage.  Each dataset in an HDF5 file can specify its own compression algorithm and associated compression factor (if appropriate) or specifiy no compression (default.)\n",
    "\n",
    "By default, no compression is specified by $\\textbf{HSP}^\\textbf{2}$.\n",
    "\n",
    "HDF5 packing tools, h5prepack.exe and ptrepack.exe, can also apply a global compression algorithm and compression factor when they repack an HDF5 file.\n",
    "\n",
    "However, we have not been able to register a standard compression algorithm, BLOSC, to HDFView and still view the data correctly. Since viewing the HDF5 files in HDFView or HDFCompass is valuable to these tutorials, we will continue to not compress the HDF5 files.  \n",
    "\n",
    "But for your own projects, you should consider compression.\n",
    "\n",
    "**NOTE:** HDF5 supports both lossy and lossless compression algorithms.\n",
    "\n",
    "**NOTE:** Data is compressed and decompressed on the fly by the internals of HDF5. As a user, you don't need to do anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fifth,  save the computed data in another format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, $\\textbf{HSP}^\\textbf{2}$ saves DataFrame tables using these options:\n",
    "\n",
    "``` data_columns=True, format='table')```\n",
    "\n",
    "These options result in tables appearing like those in these tutorials when viewed \n",
    "with HDFView or HDFCompass.  These options are necessary if the data is to be appended or queried.\n",
    "\n",
    "However, they waste a lot more space since they require a B-tree to be created to find the non contiquous data blocks. They are slower to read and write as well.\n",
    "\n",
    "Perhaps, for real world use (not tutorials), the non table format should be considered.  (The I/O time is a significant portion of the $\\textbf{HSP}^\\textbf{2}$  run time.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sixth, save project documentation to HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation created during the waterhed project can be saved to HDF5.\n",
    "\n",
    "Version control for MATLAB (.m files), Python files (.py files) and IPython Notebooks (.ipynb files) can also\n",
    "be saved to the HDF5 file.  (Mercurial version control is the recommend tool.)\n",
    "\n",
    "Other documents such as scanned data and PDF files can be saved to HDF5.\n",
    "\n",
    "**NOTE:** Documents will not be viewable with HDFView (except plain text). You can see the dataset as raw bytes (which is not very useful.) But they can be extracted from the HDF5 file and viewed normally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXAMPLE**\n",
    "\n",
    "Your directory contains a PDF file, JasonWEFTEC 2014.pdf.  We will save this file into the mystudy.h5 file and then restore it.\n",
    "\n",
    "The cells starting with \"!\" are Windows command lines.  If you are on a Linux or MAC computer, you will need to use appropriate commands to start a PDF viewer.\n",
    "\n",
    "Check PDF file is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join('TutorialData', 'JasonWEFTEC 2014.pdf')\n",
    "os.listdir('TutorialData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSP2tools.save_document(hdfname, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use HDFView to see that the document as been added. Delete Pandas.pdf in the directory, and check it is gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(filepath)\n",
    "os.listdir('TutorialData')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file does not exist.\n",
    "\n",
    "Now restore the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSP2tools.restore_document(hdfname, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('TutorialData')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that the file was not damaged by the round-trip by viewing it at this [link](TutorialData/JasonWEFTEC 2014.pdf) or opening it in your favorite PDF viewer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SECTION SUMMARY**\n",
    "\n",
    " + Demonstrated using data from the Master or Respository HDF5 files rather than duplicating it.\n",
    " + Demonstrated packing HDF5 files to make them smaller\n",
    " + Discussed HDF5 dataset compression\n",
    " + Demonstrated saving a document into an HDF5 file.\n",
    " + Demonstrated restoring a document from the HDF5 file into the current directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/HSP2.png\" />\n",
    "This Jupyter Notebook Copyright 2017 by RESPEC, INC.  All rights reserved.\n",
    "\n",
    "$\\textbf{HSP}^{\\textbf{2}}\\ \\text{and}\\ \\textbf{HSP2}\\ $ Copyright 2017 by RESPEC INC. and released under this [License](LegalInformation/License.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUTORIAL 5: $\\textbf{HSP}^\\textbf{2}$  Plotting and Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial will cover how to make publication quality plots and create reports from the HDF5 simulation data.\n",
    "\n",
    "**Tutorial Contents**\n",
    "\n",
    " + Section 1: [Plotting](#section1)\n",
    " + Section 2: [Reports](#section2)\n",
    " + Section 3: [Using Pandas for Reports](#section3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import site\n",
    "site.addsitedir(os.getcwd().rsplit('\\\\',1)[0] + '\\\\')  # adds your path to the HSP2 software.\n",
    "\n",
    "hdfname = os.path.join('TutorialData', 'Tutorial.h5')\n",
    "\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows    = 18\n",
    "pd.options.display.max_columns = 10\n",
    "pd.options.display.float_format = '{:.2f}'.format  # display 2 digits after the decimal point\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import HSP2\n",
    "import HSP2tools\n",
    "\n",
    "HSP2tools.reset_tutorial()    # make a new copy of the tutorial's data\n",
    "HSP2tools.versions()          # display version information below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Plotting<a id='section1'></a>\n",
    "\n",
    "It is easy to make professional quality plots. Pandas uses the matplotlib  library under the hood which provides essentially all the plot\n",
    "options in MATLAB (2-d only).\n",
    "\n",
    "The calculated VOLEV timeseries from Reach 1 and Reach 5 will be used to demonstrate the concepts.\n",
    "\n",
    "**NOTE** When a user specifies saving a calculated timeseries (this will be shown later), the entire timeseries is saved at the timestep of the simulation. This allows resampling at convenient timesteps (like days, weeks, months, years) for both plotting and creating written reports.  Timeseries that are needed for inputs to other modules are automatically saved. The remaining timeseries are only saved when specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSP2.run(hdfname, saveall=True)  # make sure the computed results are available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the VOLEV data from the tutorial HDF5 file for segments '1' and '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_hdf(hdfname, '/RESULTS/RCHRES_R001/HYDR')['VOLEV']\n",
    "df5 = pd.read_hdf(hdfname, '/RESULTS/RCHRES_R005/HYDR')['VOLEV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first few values to see that the timeseries is as expected and is hourly (Freq is H for hourly).\n",
    "\n",
    "**NOTE** the function **head()** used below is just a simple method to display the first few lines of data. The function **tail()** similarly shows the last few data items.  Either function can take an integer argument to specify the number of lines to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a simple plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.plot(figsize=[20, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty messy plot, but works easily.\n",
    "\n",
    "Now aggregate the data into daily time intervals and select the evaporation sum for each day for a cleaner plot.\n",
    "\n",
    "**Note** The aggregation method is \"sum\" which adds up all the values falling into the new time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_dailysum = df1.resample('D', how='sum')\n",
    "df1_dailysum.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_dailysum.plot(figsize=[20, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maybe a weekly aggregation is less messy, but still shows the data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_weeklysum = df1.resample('W', how='sum')\n",
    "df1_weeklysum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_weeklysum.plot(figsize=[20, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OK, better. Now do this for the REACH 5 timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_weeklysum = df5.resample('W', how='sum')\n",
    "df5_weeklysum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a plot using both timeseries with labels and legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df1_weeklysum.plot(style='red',  label='REACH1', figsize=[20, 10]) \n",
    "df5_weeklysum.plot(style='blue', label='REACH5')\n",
    "plt.title('Weekly Sum of Evaporation, VOLEV')    # can set titles\n",
    "plt.ylabel('acre-ft/ hour')                      # can set axis labels\n",
    "plt.legend(loc='best')                           # make a legend\n",
    "\n",
    "# demonstrate saving the plots to files in various format\n",
    "plt.savefig('tutorial5.png')\n",
    "plt.savefig('tutorial5.pdf')\n",
    "plt.savefig('tutorial5.tiff')\n",
    "plt.savefig('tutorial5.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas plot() makes prettier time axis than matplotlib directly, but you can use matplotlib routines for other the options like legends and titles.\n",
    "\n",
    "The matplotlib library generally has every option that MATLAB has for 2-d plotting with similar names and arguments. This makes it easy to generate publication quality plots.\n",
    "\n",
    "You can open the PNG, PDF, TIFF, and SVG files in the TutorialsData directory and view them with your viewing tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some background on interactive plotting\n",
    "\n",
    "There is very active development going on to visualize \"big\" data (including plotting) interactively. This is being driven by big science and also by \"Quants\", Quantitative Analysts and Data Scientists (new fields) seeking to process a lot of data fast and visualize it quickly in order to make stock market or trading decisions.\n",
    "These development projects all rely  on advanced HTML 5 features. Different browsers have different levels of compliance to HTML 5, so sometimes some tool features break on any specific browser.\n",
    "\n",
    "Some links to a very few of the many interesting projects:\n",
    "    \n",
    " + [D3](http://d3js.org/) This is the great grand daddy of these efforts. the weird looking display of hexagons are selected examples. Click on one and see it demonstrated.\n",
    " + [Ipython Vega](https://github.com/vega/ipython-vega) This project is under active development to bring Vega to the Jupyter Notebook.\n",
    " + [plotly](https://plot.ly/). This was a commercial product - but recently became free to use.\n",
    " + [bokeh](http://bokeh.pydata.org/) Continuum Analytics won a large (3 million dollar) grant from DARPA's XDATA project. It requires all the funded projects to become open source. It is D3 like in look & feel, but very different visualization grammar. Click on one of the images to see it work and to see the code. (Most seem to be interactive examples, but not all.) Well integrated with IPython Notebook.\n",
    " + [mpld3](http://mpld3.github.io/)  This project which converts matplotlib calls to D3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Matplotlib to make interactive plots\n",
    "\n",
    "A feature of the 1.4 matplotlib is the new notebook backend to make it easy to make interactive plots.\n",
    "Put this in your Python imports and settings cell:\n",
    "\n",
    "> %matplotlib notebook\n",
    "\n",
    "instead of \n",
    "\n",
    "> %matplotlib inline\n",
    "\n",
    "Note the tools to pan, zoom, and reset the plot. Hover over a tool icon and a hint will appear to describe that tool. You can zoom into specific hourly data and pan smoothly to any part of the timeseries. Hover your mouse over the plotted data and the time and value are displayed below the plot frame.\n",
    "\n",
    "When you are done, click on the x in the red box at the upper right corner of the plot which freezes the plot in the current view and exits from the interactive mode in that cell. \n",
    "\n",
    "Here is an example using an hourly precpitation time series.\n",
    "\n",
    "**Note** the Jupyter Notebook is not really good at allowing changes to the matplotlib backend once the Notebook is started. \n",
    "\n",
    "Currently, you should not mix backends. But for this tutorial as the last plot, we will try this.\n",
    "\n",
    "(It might fail. In this case, you need to remove the inline statement near the top of this notebook and replace it with the matplotlib notebook.  Save the notebook and restart the kernel. Then it should work.  Active development is being made to embed the plotting package into the Jupyer Notebook in a way to allow more dynamic changes of the backend.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "df1 = pd.read_hdf(hdfname, 'TIMESERIES/TS39')\n",
    "df1.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section Summary**\n",
    "\n",
    " + Demonstrated disaggregation of the data from hourly to daily and weekly intervals\n",
    " + Demonstrated making plots with\n",
    "   + multiple timeseries\n",
    "   + plot Title\n",
    "   + plot Axis Labels\n",
    "   + control of line styles (color)\n",
    "   + Plot legend\n",
    " + Demonstrated interactive plotting\n",
    " + Demostrated saving plots in a variety of formats\n",
    "   + SVN\n",
    "   + PNG\n",
    "   + PDF\n",
    "   + TIFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Reports<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the computed time series datasets are needed for this example. Use the saveall option to insure the necessary data are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSP2.run(hdfname, saveall=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate creating report like traditional HSPF\n",
    "\n",
    "The following is part of HSPF test 10 output.  This tutorial will show how to make a similar report - but covering all months at once."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PERVIOUS LAND SEGMENT NO.   1                           BICKNELL FARM                                     REPORT FOR MONTH 1976/ 1\n",
    "\n",
    " *** SNOW ***\n",
    "\n",
    "   STATE VARIABLES                   PACK     PACKF     PACKW     PACKI    PDEPTH    COVINX    NEGHTS    XLNMLT\n",
    "                                       IN        IN        IN        IN        IN        IN        IN        IN\n",
    "                                     0.19      0.18      0.00      0.18      0.20      0.50      0.00      0.00\n",
    "\n",
    "                                   RDENPF    SKYCLR    SNOCOV      DULL    ALBEDO    PAKTMP    DEWTMP   SNOWTMP\n",
    "                                                                                      DEG F     DEG F     DEG F\n",
    "                                     0.93      0.15      0.37     787.5      0.60      29.8      17.0      32.3\n",
    "\n",
    "   FLUXES                          PRECIP     SNOWF     PRAIN     SNOWE    WYIELD      MELT\n",
    "                                       IN        IN        IN        IN        IN        IN\n",
    "                                     0.17      0.25      0.00      0.08      1.68      3.76\n",
    "\n",
    "   BALANCE                      % ERROR IN   INCHES\n",
    "                                  0.000   1.947E+00\n",
    "\n",
    " *** PWATER ***\n",
    "\n",
    "   STATE VARIABLES                   PERS      CEPS      SURS       UZS      IFWS       LZS      AGWS      GWVS    INFFAC    PETADJ\n",
    "                                       IN        IN        IN        IN        IN        IN        IN        IN\n",
    "                         \n",
    "5.812     0.036     0.000     0.193     0.000     5.434     0.150     0.128     0.817     0.000\n",
    "\n",
    "   FLUXES\n",
    "     EXTNL INFLOWS and OUTFLOWS  MOISTURE<-----OUTFLOWS TO STREAM---------->(SUM) DEEP PERC\n",
    "                                     SUPY      SURO      IFWO      AGWO      PERO      IGWI\n",
    "                                       IN        IN        IN        IN        IN        IN\n",
    "                                    1.679     0.001     0.000     0.079     0.080     0.020\n",
    "\n",
    "     EVAPOTRANSPIRATION         POTENTIAL<--------------------ET COMPONENTS-------------------->(SUM)\n",
    "                                      PET      CEPE      UZET      LZET     AGWET     BASET      TAET\n",
    "                                       IN        IN        IN        IN        IN        IN        IN\n",
    "                                    0.016     0.016     0.000     0.000     0.000     0.000     0.016\n",
    "\n",
    "     INTERNAL FLUXES                 IFWI       UZI     INFIL      PERC       LZI      AGWI\n",
    "                                       IN        IN        IN        IN        IN        IN\n",
    "                                    0.000     0.043     1.633     0.000     1.434     0.179\n",
    "\n",
    "   BALANCE                      % ERROR IN   INCHES\n",
    "                                  0.000   5.929E+00\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERLND SNOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowpath = '/RESULTS/PERLND_P001/SNOW'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snow - State Variables\n",
    "\n",
    "The following defines the state variables in the order they should occur in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snow_state_variables = ['PACKF', 'PACKW', 'PACKI','PDEPTH','COVINX', 'NEGHTS', 'XLNMLT', 'RDENPF',\n",
    " 'SKYCLR', 'SNOCOV',  'DULL', 'ALBEDO', 'PAKTMP', 'DEWTMP', 'SNOTMP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following reads the entire snow data from the HDF5 file, then selects only the state variables defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowsv = pd.read_hdf(hdfname, snowpath)[snow_state_variables]\n",
    "snowsv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert the frequency to \"monthly\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowsv = snowsv.resample('M', how='last')\n",
    "snowsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the HSP2 answers for the same simulation test in the table above including the same number of digits.\n",
    "They following shows how this might print.  The first column could be \"prettyed up\" to show just the month or month/year as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print snowsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print directly to a file almost as easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TutorialData/print_example.txt', 'w') as f:\n",
    "    print >>f, snowsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open this file with any text editor or view in your [browser](TutorialData/print_example.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNOW - Fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snow_fluxes = ['SNOWF', 'PRAIN', 'SNOWE',  'WYIELD', 'MELT', 'RAINF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_flx = pd.read_hdf(hdfname, snowpath)[snow_fluxes]\n",
    "snow_flx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again convert to Monthly frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_flx = snow_flx.resample('M', how='sum')\n",
    "snow_flx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERLND WATER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WATER - State Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "waterpath = '/RESULTS/PERLND_P001/PWATER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_state_variables = ['PERS', 'CEPS', 'SURS', 'UZS', 'IFWS', 'LZS', 'AGWS', 'GWVS', 'INFFAC', 'PETADJ', 'TGWS']\n",
    "watersv = pd.read_hdf(hdfname, waterpath)[water_state_variables].resample('M', how='last')\n",
    "watersv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Water - Fluxes (External)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.3f}'.format  # display 3 digits after the decimal point\n",
    "\n",
    "water_fluxes_external = ['SUPY', 'SURO', 'IFWO', 'AGWO',  'PERO','IGWI', 'SURI']\n",
    "water_flx_external = pd.read_hdf(hdfname, waterpath)[water_fluxes_external].resample('M', how='sum')\n",
    "water_flx_external"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Water - Fluxes  (Evapotranspiration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_fluxes_ev = ['CEPE', 'UZET',  'LZET', 'AGWET', 'BASET', 'TAET']\n",
    "water_flx_ev = pd.read_hdf(hdfname, waterpath)[water_fluxes_ev].resample('M', how='sum')\n",
    "water_flx_ev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Water - Fluxes  (Internal Fluxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_fluxes_internal =  ['IFWI', 'UZI',  'INFIL', 'PERC',  'LZI', 'AGWI', 'SURI']\n",
    "water_flx_internal = pd.read_hdf(hdfname, waterpath)[water_fluxes_internal].resample('M', how='sum')\n",
    "water_flx_internal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch name of this segment from the HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment = 'P001'\n",
    "name = pd.read_hdf(hdfname, '/PERLND/GENERAL_INFO').loc[segment, 'LSID']\n",
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now put this together into one report.\n",
    "\n",
    "Change display options for printed page rather than this Notebook display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 15)\n",
    "pd.set_option('display.width', 150)\n",
    "pd.set_option('display.show_dimensions', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TutorialData/print_example2.txt', 'w') as f:\n",
    "    print >>f, 'PERVIOUS Land Segment', segment, name\n",
    "    print >>f\n",
    "    print >>f, 'SNOW State Variables'\n",
    "    print >>f\n",
    "    print >>f, snowsv\n",
    "    print >>f\n",
    "    print >>f, 'SNOW Fluxes'\n",
    "    print >>f\n",
    "    print >>f, snow_flx\n",
    "    print >>f\n",
    "    print >>f, 'PWATER State Variables'\n",
    "    print >>f\n",
    "    print >>f, watersv\n",
    "    print >>f\n",
    "    print >>f, 'PWATER FLUXES'\n",
    "    print >>f\n",
    "    print >>f, 'External Inflows and Outflows'\n",
    "    print >>f\n",
    "    print >>f, water_flx_external\n",
    "    print >>f\n",
    "    print >>f, 'Evapotranspiration Potential'\n",
    "    print >>f\n",
    "    print >>f, water_flx_ev\n",
    "    print >>f\n",
    "    print >>f, 'Internal Fluxes'\n",
    "    print >>f\n",
    "    print >>f, water_flx_internal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the report file, print_example2.txt, with any text editor or view it in your [browser](TutorialData/print_example2.txt)\n",
    "\n",
    "Additional formatting or content can customize the report as necessary.\n",
    "\n",
    "The user can create Python files for specific report needs (including plots) and run them in one step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Using Pandas for Reports<a id='section3'></a>\n",
    "\n",
    "Pandas is useful in \"munging\" data into the required form for reports. These tutorials have demonstrated converting timeseries data from hourly data into annual, monthly, and weekly intervals with various aggregation methods including max, sum, and last.\n",
    "\n",
    "The following image shows the basic intervals available for reports. Multiples of these intervals are also available. For example, 'H' is an hourly interval and '4H' is every4 hours.  Pandas takes care of leap years, time zones, and other similar issues.\n",
    "\n",
    "<img src=\"files/Images/FrequencyConversion.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequently reports need intervals based on different starting dates. For example, to create an annual report from June to June.\n",
    "These are called Anchored Offsets.  For example, 'A-OCT' is an annual frequence starting and ending at the end of October (October 31).\n",
    "\n",
    "<img src=\"files/Images/AnchoredOffsets.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many options for aggregating/disagregating the data. For example, these are the available interpolation methods:\n",
    "\n",
    " + linear\n",
    " + time\n",
    " + values\n",
    " + index\n",
    " + nearest\n",
    " + zero\n",
    " + slinear\n",
    " + quadratic\n",
    " + cubic\n",
    " + barycentric\n",
    " + krogh\n",
    " + polynomial\n",
    " + spline\n",
    " + piecewise_polynomial\n",
    " + pchip\n",
    "    \n",
    "\n",
    "The \"how\" function in the resample() can be any  numpy array function or user defined array function including\n",
    "\n",
    " + abs\n",
    " + max\n",
    " + min\n",
    " + mean (default)\n",
    " + std\n",
    " + last\n",
    " + pad (forward and backward)\n",
    " + sum, nansum\n",
    " + prod\n",
    " + cumsum\n",
    " + cumprod\n",
    " + diff\n",
    " \n",
    "Users can specify which endpoint (right or left) is included in the intervals\n",
    "\n",
    "Users can use many database operations between Pandas tables including\n",
    "\n",
    " + merges (inner, outer, left, right)\n",
    " + groupby\n",
    " + concat, join, append\n",
    " + pivot\n",
    " + multiindex\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Other Reporting Options\n",
    "\n",
    "The IPython Notebook has a tool **nbconvert** that can convert a notebook into HTML, $\\LaTeX$, PDF, and slide show (jsRevel) formats. Look under the \"File\" menu at the top of the page. Select it and then select \"Download as\" to see some options.  In order to convert to some formats, additional software needs to be installed. For example, to convert to $\\LaTeX$ or PDF, $\\TeX$ installation (such as MikTex) must be installed. You can try converting this notebook to HTML (since everything is available) to try this feature.\n",
    "\n",
    "\n",
    "The nbcovert tool\n",
    "can also be run from a command prompt window with more options. It is a bit brittle in creating LaTeX and PDF files, but\n",
    "works. The PDF is created from an intermediate LaTex representation - so it looks really good. The documentation is at [nbconvert](http://ipython.org/ipython-doc/stable/notebook/nbconvert.html). Pandas and Bokeh are improving the **repr** representation of their\n",
    "objects(plots, timeseries, tables, etc.) for better display/printing in HTML and LaTex. \n",
    "\n",
    "View the PDF files created from Tutorial 1 and Tutorial 3 to see the results of nbconvert. (These files are located in the ExampleData directory.)  A few items are either missing or are a bit messed up, but the overall documents only need a bit of tweaking for release. The corresponding $\\TeX$ files are not included. Assuming that the command window has the current directory set to the directory running these tutorials the command to create a PDF file is\n",
    "\n",
    "    ipython nbconvert --to PDF \"Tutorial 1.ipynb\"\n",
    "  \n",
    "and to create a reveal slideshow (and start it running) is\n",
    "\n",
    "    ipython nbconvert --to slides --post serve \"Tutorial 1.ipynb\"\n",
    "  \n",
    "The name of the file had to be in quotes because of the space in the name.\n",
    "\n",
    "Due in part to the Sloan Foundation Grant and Microsoft Grant, the IPython team is rapidly improving the notebook capabilities to\n",
    "generate documentation in many formats.  The [IPython Example Notebook View Page](http://nbviewer.ipython.org/) show the Notebook supporting 3 languages and\n",
    "3 books written in the Notebook.  The [Gallery](https://github.com/ipython/ipython/wiki/A-gallery-of-interesting-IPython-Notebooks)\n",
    "shows many more examples including a growing number of Acedemic publications.\n",
    "\n",
    "New publishing concepts are rapidly developing. [Authorea](https://www.authorea.com/)  is a service to create technical articles which now\n",
    "supports IPython Notebooks.\n",
    "\n",
    "So hydrologists using Notebooks to run HSP2 can actually use their Notebooks to record their research and workflow, then generate documentation directly from these Notebooks. A great example of professional hydrology notebooks are by Dr. Jeffrey Kantor for the [Rainly Lake region](http://jckantor.github.io/Rainy-Lake-Hydrology/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

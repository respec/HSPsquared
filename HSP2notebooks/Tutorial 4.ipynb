{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/HSP2.png\" />\n",
    "This Jupyter Notebook Copyright 2016 by RESPEC, INC.  All rights reserved.\n",
    "\n",
    "$\\textbf{HSP}^{\\textbf{2}}\\ \\text{and}\\ \\textbf{HSP2}\\ $ Copyright 2016 by RESPEC INC. and released under this [License](LegalInformation/License.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUTORIAL 4: Working with Legacy HSPF files and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial notebook demonstrates how to use legacy UCI and WDM files to create\n",
    "an HDF5 file for $\\textbf{HSP}^{\\textbf{2}}$. It shows how to use legacy PLTGEN files to plot results (like the plots in Tutorial 5.) Finally, it demonstrates how to perform a post run analysis like the HSPF DURANL.\n",
    "\n",
    "**Tutorial Contents**\n",
    "\n",
    " + Section 1: [Importing UCI Files into HDF5](#section1)\n",
    " + Section 2: [Importing WDM Files into HDF5](#section2)\n",
    " + Section 3: [HSP2 DURANL Functionality](#section3)\n",
    " + Section 4: [HSP2 PLTGEN Functionality](#section4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Python imports  and setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import site\n",
    "site.addsitedir(os.getcwd().rsplit('\\\\',1)[0] + '\\\\')  # adds your path to the HSP2 software.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows    = 18\n",
    "pd.options.display.max_columns = 20\n",
    "pd.options.display.float_format = '{:.2f}'.format  # display 2 digits after the decimal point\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "\n",
    "import HSP2\n",
    "import HSP2tools\n",
    "\n",
    "HSP2tools.reset_tutorial()    # make a new copy of the tutorial's data\n",
    "HSP2tools.versions()          # display version information below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Set the filenames\n",
    "This first example uses the HSPF test 10 UCI and WDM files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uciname = os.path.join('TutorialData', 'test10.uci') \n",
    "wdmname = os.path.join('TutorialData', 'test.wdm')\n",
    "\n",
    "unpackedhdfname = os.path.join('TutorialData', 'unpackedtutorial.h5')\n",
    "hdfname = os.path.join('TutorialData', 'Tutorial.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: make sure the tutorial HDF5 files does not exist at the start of this tutorial (since we want to make it here!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(hdfname)\n",
    "os.listdir('TutorialData')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Importing UCI Files into HDF5<a id='section1'></a>\n",
    "\n",
    "HSPF was developed for a user input file (the UCI file) based on 80 column punch cards.\n",
    "The format of data on each card was specific to the type of data it contained.\n",
    "The sequence files for PERLND, IMPLND, and REACHES contain the format specifications to read the text file lines along\n",
    "with other information such as default values (for unspecified data), maximum and minimum limits per data element, message\n",
    "strings defining the meaning/use of each data element, and units type (English or Metric.)\n",
    "The UCI Reader uses these sequence files to parse the user's UCI file.\n",
    "\n",
    "The $\\textbf{HSP}^\\textbf{2}$  UCI Reader function currently is mostly complete except for a few tables that span multiple \"cards\" or have a special context.  The readUCI module will be fixed to read these tables when the associated HSPF modules are converted to $\\textbf{HSP}^\\textbf{2}$. \n",
    "\n",
    "The UCI Reader will create the HDF5 file if necessary.\n",
    "If the HDF5 file already exists, it overwrites the UCI corresponding information.\n",
    "\n",
    "The HDF5 file includes the UCI file information (except obsolete elements) plus some new tables.  For example, there is now a SAVE table for each module in PERLND,\n",
    "IMPLND, and REACHES that specifies almost every computed timeseries and each segment. A one in the intersection\n",
    "of a named timeseries and a segment will save the results to the HDF5 file, otherwise it is not saved. This allows fine control for saving results for post run analysis. By default, only the output flux timeseries are saved.\n",
    "This tutorial shows how to save everything as an example of modifying the SAVE tables from the default.\n",
    "\n",
    "A few timeseries which can be trivially computed from the other timeseries are not explictly named or saved. The View Perlnd, View Implnd, and View Reaches notebooks provide examples of calculating the \"missing\" timeseries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the  UCI Reader\n",
    "    \n",
    "The following cell creates the HDF5 file and populates it with the data from the UCI file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSP2tools.makeH5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HSP2tools.readUCI(uciname, unpackedhdfname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use HDFView or HDFCompass to examine the resulting HDF5 file if you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** If the user is only interested in using the Network Tool on a legacy watershed,\n",
    "then you can stop here. The Network Tool (Tutorial 6) can be run using this HDF5 file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** The readUCI routine can take two options.\n",
    "+ HSPF - (optional, defaults to False)\n",
    "   + When HSPF is True, all the data from the UCI file is imported - even if it is not used by $\\textbf{HSP}^\\textbf{2}$ \n",
    "   + When HSPF is False, the obsolete data from the UCI file is ignored.\n",
    " \n",
    "+ metric - (optional, defaults to False).\n",
    "   + When metric is True, the UCI data is read as being in metric units.\n",
    "   + When metic is False, the UCI data is read as being in English units.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section Summary**\n",
    "\n",
    " + demonstrated how to create an HDF5 file for HSP$^2$ by reading a legacy UCI file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Importing WDM Files into HDF5<a id='section2'></a>\n",
    "\n",
    "The WDM reader must be run after the UCI reader because it uses the EXT_SOURCES table from the HDF5 file to determine\n",
    "which timeseries to extract from the WDM file. It also extracts some metadata from the EXT_SOURCES table which is attached as attributes when the time series is saved to the HDF5 file.\n",
    "\n",
    "For each timeseries named in the EXT_SOURCES table, the entire timeseries is extracted from the WDM file. It is placed into a Pandas Series and then saved to the HDF5 file.\n",
    "\n",
    "Each time series WDM name is converted from a number to a string prefaced by \"TS\". The UCI reader has already adjusted all references to the time series datasets to use this naming convention.\n",
    "\n",
    "The WDM reader also extracts the metadata from the WDM file and attaches it to the timeseries as attributes.  Pandas also attaches its own metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits, License and Copyright for wdmtoolbox\n",
    "\n",
    "The UCI Reader uses Tim Cera's **wdmtoolbox**.  This code based on version 0.8.2 was modified by RESPEC because HSPF UCI files that didn't contain a Constituent, Location, or Scenario attrributes will terminate with an error. Since the default HSPF test files don't have these attributes,  HSP2 can't run the test cases without this fix. This was reported to Tim.\n",
    "\n",
    "The wdmtoolbox is released under a BSD license and Tim Cera retains all rights to his module except as explicitly granted by the license.\n",
    "\n",
    "The wdmtoolbox is only used by the $\\textbf{HSP}^\\textbf{2}$  readWDM module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the  WDM Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSP2tools.ReadWDM(wdmname, unpackedhdfname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pack the HDF5\n",
    "\n",
    "\n",
    "You should make the file smaller by repacking it with ptrepack.exe as discussed in Tutorial 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ptrepack {unpackedhdfname}  {hdfname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the HDF5 file with HDFView or HDFCompass - Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Close and reopen) HDFView to examine the HDF5 file, tutorial.h5,  which is now complete and ready to run a simulation.  \n",
    "\n",
    "Using HDFView, click on the TS39 timeseries and look at the\n",
    "metadata. You can drag the boundary of the metadata panal up to make it bigger or just scroll.  \n",
    "\n",
    "The WDM file doesn't attach real units to its data. The **EXT_SOURCES** has\n",
    "a units column (from the UCI data), but it is a string like 'ENGL'.\n",
    "It is stored in the metadata as 'wdm_units'.  \n",
    "\n",
    "The 'ENGL' designation is not an accepted units measurement and is also ambiquous.\n",
    "For example the rainfall timeseries is probably in inches per hour, but internally it is converted to feet per time interval. If the rainfall series comes from a daily source and is disaggregated (via \"DIV\"), then the units of the original timeseries (inch per day) is different than the converted. \n",
    "\n",
    "The real units are marked as '???'. It would be a good practice to annotate the actual units when data is imported. \n",
    "\n",
    "Ideally, the new HSP2 could examine the actual units of a timeseries to perform any required aggregation/disaggregation, and mark the resulting timeseries with the new units.  Units would be carried with the data throughout HSP2. But would require the user to alway set the actual units. This was demonstrated in Tutorial 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run HSPF test10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now show that this HDF5 file can run the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSP2.run(hdfname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calleg Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calleg is a real watershed and has\n",
    "+ 27 IMPLND segments,\n",
    "+ 129 PERLND segments,\n",
    "+ 119 RCHRES segments,\n",
    "+ 9 years of simulation time with hourly time steps (78,888 timesteps.)\n",
    "\n",
    "This is watershed is a more realistic indication of HSP$^2$ performance than test10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uciname = os.path.join('TutorialData', 'calleg.uci') \n",
    "wdmname = os.path.join('TutorialData', 'calleg.wdm')\n",
    "\n",
    "unpackedhdfname = os.path.join('TutorialData', 'unpackedcalleg.h5')\n",
    "hdfname = os.path.join('TutorialData', 'calleg.h5.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the steps above for this watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(hdfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSP2tools.readUCI(uciname, unpackedhdfname)\n",
    "HSP2tools.ReadWDM(wdmname, unpackedhdfname)\n",
    "\n",
    "!ptrepack {unpackedhdfname}  {hdfname}\n",
    "!del {unpackedhdfname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run this simulation. This larger simulation will take a few minutes depending on your computer's speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HSP2.run(hdfname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section Summary**\n",
    "\n",
    " + demonstrated how to import timeseries from a WDM file into an HDF5 file\n",
    " + demonstrated that the new HDF5 file can run the watershed functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: HSP2 DURANL Functionality<a id='section3'></a>\n",
    "\n",
    "Pandas makes it easy to answer questions like those in the traditional HSPF DURANL Analysis.\n",
    "These techniques can be used on any timeseries, external source or computed.\n",
    "\n",
    "Basically, DURANL answers questions such as\n",
    "\n",
    " + Fraction of time spent spent in exceeding a given level for excursions (runs) greater than a specified duration,\n",
    " + Time spend between specified levels for excursions greater than a specified duration,\n",
    " + Number of excursions exceeding a specified level for excursions greater than a specified duration,\n",
    " + Average duration of excursions exceeding a specified level for excursions greater than a specified durations,\n",
    " + Standard deviation of duration of excursions exceeding a specified level with durations greater than a specified duration.\n",
    " \n",
    "This is not an exhaustive list of DURANL functions, but provides the flavor of its capability.\n",
    " \n",
    "Perform this analysis (post run) using Pandas with these general opeations:\n",
    "\n",
    " + Read timeseries from HDF5 file using Pandas.\n",
    " + Aggregate/Disaggregate to the desired time steps (generally daily, monthly, annually) possibly using anchored values (such as annual starting in June),\n",
    " + Truncate to desired time period if necessary,\n",
    " + Use Pandas to create boolean arrays answering the question of levels,\n",
    " + Use a function such as runlength (shown below) to count the duration of an excursion,\n",
    " + Use Pandas tools such as histograms, selecting values above a certain duration, statistics.\n",
    "\n",
    "The following example uses a temperature timeseries in test10.h5\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example function to calculate run lengths, returns sorted count list of run lengths as Pandas series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runlength(boolean):\n",
    "    durations = []\n",
    "    counter   = 0\n",
    "    \n",
    "    for b in boolean:\n",
    "        if b:\n",
    "            counter += 1\n",
    "        elif counter > 0:\n",
    "            durations.append(counter)\n",
    "            counter = 0\n",
    "    \n",
    "    # catch run at end that doesn't return to zero\n",
    "    if counter > 0:\n",
    "        durations.append(counter)\n",
    "        \n",
    "    return pd.Series(sorted(durations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfname = os.path.join('TutorialData', 'Tutorial.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_hdf(hdfname, 'TIMESERIES/TS122')\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create boolean expersion for exceeding levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean = ds > 32.0\n",
    "boolean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  more complex expressions can represent excursion between two levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b =  (32.0 < ds) & (ds <= 50.0)\n",
    "\n",
    "b.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now answer DURANL like questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of time above this level (32.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 100.0 * sum(boolean)/len(boolean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of time between 32.0 and 50.0 (inclusive). (Note: this uses the b array computed above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 100.0 * sum(b)/len(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate all the runs (count consecutive boolean True values). Returns sorted Pandas series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = runlength(boolean)\n",
    "\n",
    "print runs.head()\n",
    "print\n",
    "print runs.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show histogram of run lengths (for excursions > 32.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exclude last big run value (summer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(runs[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many runs greater than 50 in duration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print sum(runs >  50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of runs with duration between 50 and 100 remembering these represent excursions above 32.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print sum( (runs > 50) & (runs < 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics on durations for excusion above 32.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs[(runs > 50) & (runs < 100)].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section Summary**\n",
    "\n",
    "  + demonstrated that the HDF5 files can be easily processed (post run) to answer DURANL like questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: HSP2 PLTGEN Functionality<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLTGEN File Format Assumptions**\n",
    "\n",
    " + Text, not binary file\n",
    " + Initial 4 characters can be ignored in each line\n",
    " + First 25 lines are header information\n",
    " \n",
    "To find the column header information\n",
    "\n",
    " + The line containing the word \"LINTYP\" immediately proceeds the column header lines\n",
    " + The column headers stop at the first of\n",
    "     + line 26\n",
    "     + blank line (ignoring the first 4 characters)\n",
    "     + finding a line starting with \"Time series\" (ignoring the first 4 characters)\n",
    "\n",
    "To find the data (columns of time series data)\n",
    "\n",
    " + Line 26 is dummy data\n",
    " + Line 27 and on are actual lines with time series data\n",
    " + No entry is blank => all lines have the same number of entries (columns)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read a PLTGEN file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pltgen = os.path.join('TutorialData', 'rch900.6')\n",
    "df = HSP2tools.readPLTGEN(pltgen)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the units are great, they make a complicated column header for working with the\n",
    "data and the column names do not follow the Natural Naming convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FLOW (ft3/s)'].plot(figsize=[20,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing a column from the DataFrame (like above), produces a time series\n",
    "(Pandas Series). This allows resampling to other periods such as monthly and\n",
    "annually (as shown in Tutorial 3). The Pandas resampling methods include mean, sum, last, first, max, and min which cover the PLTGEN methods.  (PLTGEN AVER is Pandas mean.)  Pandas actually provides many more methods and allows user defined methods.\n",
    "\n",
    "Calculate the annual flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts =  df['FLOW (ft3/s)']\n",
    "ts = ts.resample('A').sum()\n",
    "ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas can work with the entire DataFrame without needing to extract individual time series. Most functions also work with NAN's for missing data (like MATLAB.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the count above reflects the real values by ignoring NaNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section Summary**\n",
    "\n",
    " + demonstrated that HSP2 can read legacy PLTGEN files into Pandas DataFrames for analysis, plotting, and reporting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
